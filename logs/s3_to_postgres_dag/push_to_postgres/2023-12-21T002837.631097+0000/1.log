[2023-12-21 00:28:41,568] {taskinstance.py:1043} INFO - Dependencies all met for <TaskInstance: s3_to_postgres_dag.push_to_postgres manual__2023-12-21T00:28:37.631097+00:00 [queued]>
[2023-12-21 00:28:41,578] {taskinstance.py:1043} INFO - Dependencies all met for <TaskInstance: s3_to_postgres_dag.push_to_postgres manual__2023-12-21T00:28:37.631097+00:00 [queued]>
[2023-12-21 00:28:41,579] {taskinstance.py:1249} INFO - 
--------------------------------------------------------------------------------
[2023-12-21 00:28:41,579] {taskinstance.py:1250} INFO - Starting attempt 1 of 1
[2023-12-21 00:28:41,579] {taskinstance.py:1251} INFO - 
--------------------------------------------------------------------------------
[2023-12-21 00:28:41,590] {taskinstance.py:1270} INFO - Executing <Task(PythonOperator): push_to_postgres> on 2023-12-21 00:28:37.631097+00:00
[2023-12-21 00:28:41,595] {standard_task_runner.py:52} INFO - Started process 19829 to run task
[2023-12-21 00:28:41,597] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 's3_to_postgres_dag', 'push_to_postgres', 'manual__2023-12-21T00:28:37.631097+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/first_dag.py', '--cfg-path', '/tmp/tmpm_3nzlvt', '--error-file', '/tmp/tmpvhxtqsfd']
[2023-12-21 00:28:41,598] {standard_task_runner.py:80} INFO - Job 36: Subtask push_to_postgres
[2023-12-21 00:28:41,644] {logging_mixin.py:109} INFO - Running <TaskInstance: s3_to_postgres_dag.push_to_postgres manual__2023-12-21T00:28:37.631097+00:00 [running]> on host eddfb0464777
[2023-12-21 00:28:41,684] {taskinstance.py:1448} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=s3_to_postgres_dag
AIRFLOW_CTX_TASK_ID=push_to_postgres
AIRFLOW_CTX_EXECUTION_DATE=2023-12-21T00:28:37.631097+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-12-21T00:28:37.631097+00:00
[2023-12-21 00:28:42,095] {first_dag.py:50} INFO - Columns in dataframe: Index(['STORE_ID', 'STORE_LOCATION', 'PRODUCT_CATEGORY', 'PRODUCT_ID', 'MRP',
       'CP', 'DISCOUNT', 'SP', 'Date'],
      dtype='object')
[2023-12-21 00:28:42,176] {first_dag.py:64} INFO - clean_store_transactions table has been created
[2023-12-21 00:28:42,177] {first_dag.py:70} INFO - Inserted 100 records into clean_store_transactions table.
[2023-12-21 00:28:43,018] {first_dag.py:85} INFO -     store_id store_location product_category  ...  discount     sp         dat
0     YR7220       New York      Electronics  ...      1.86  29.14  2019-11-26
1     YR7220       New York        Furniture  ...      1.50  13.50  2019-11-26
2     YR7220       New York      Electronics  ...      4.40  83.60  2019-11-26
3     YR7220       New York          Kitchen  ...      3.64  87.36  2019-11-26
4     YR7220       New York          Fashion  ...      2.55  82.45  2019-11-26
..       ...            ...              ...  ...       ...    ...         ...
195   YR6219       New York          Kitchen  ...      3.15  59.85  2019-11-26
196   YR6219       New York        Education  ...      0.75  14.25  2019-11-26
197   YR6219       New York        Furniture  ...      1.46  71.54  2019-11-26
198   YR6219       New York          Kitchen  ...      1.65  53.35  2019-11-26
199   YR6219       New York        Furniture  ...      5.13  51.87  2019-11-26

[200 rows x 9 columns]
[2023-12-21 00:28:43,026] {python.py:175} INFO - Done. Returned value was: None
[2023-12-21 00:28:43,037] {taskinstance.py:1288} INFO - Marking task as SUCCESS. dag_id=s3_to_postgres_dag, task_id=push_to_postgres, execution_date=20231221T002837, start_date=20231221T002841, end_date=20231221T002843
[2023-12-21 00:28:43,096] {local_task_job.py:154} INFO - Task exited with return code 0
[2023-12-21 00:28:43,123] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
